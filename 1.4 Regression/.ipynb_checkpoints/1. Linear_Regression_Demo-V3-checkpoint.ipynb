{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOSTON HOUSING DEMO:Linear Regression with Python using Scikit-Learn\n",
    "\n",
    "Dear Class,<br>\n",
    "\n",
    "We are going to solve a real world problem using the Boston Housing dataset. Our task is to built a machine learning model to predict the housing price in Boston area. This housing dataset is a part of scikit-learn and also available on kaggle for you to download. A .csv file is also included in the course material. \n",
    "[Boston Housing Dataset on kaggle](https://www.kaggle.com/heptapod/uci-ml-datasets/data). Let's use use the one which is already included in the scikit-learn dataset repository, so that, you get to know the process to load the built-in datasets from scikit-learn as well.  \n",
    "\n",
    "## A Data Science Case Study\n",
    "You are hired by a real estate company to help them in their business goals. The company wants you to predict the housing prices in Boston area. Based on the community and safety issues, some areas are in demand. The company is interested in some kind of automated way of suggesting the price of a house based on its features.<br>\n",
    "\n",
    "### You have three potential clients who want to know what prices they can expect for the homes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align='left' width='100%'> <tr> <th>Feature</th><th>\tClient 1</th><th>\tClient 2</th><th>\tClient 3</th></tr>\n",
    "\n",
    "<tr> <td>Total number of rooms in home\t</td><td> 5 rooms</td><td>\t4 rooms</td><td>8 rooms</td></tr>\n",
    "<tr> <td>Neighborhood poverty level (as %)</td><td>\t17%</td><td>\t12%\t</td><td>3%</td></tr>\n",
    "<tr> <td>Student-teacher ratio of nearby schools</td><td>\t15-to-1</td><td>\t22-to-1</td><td>\t12-to-1</td></tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='boston.jpeg' width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston House Prices dataset\n",
    "===========================\n",
    "\n",
    "You are given a dataset that contain features such as crime rate by town, proportion of residential land, nitric oxide concentration, age of the house, property tax so on.... <br>\n",
    "\n",
    "You are happy to help because you got a job to do!<br>\n",
    "\n",
    "Now, when you look at the dataset, you think that the linear regression is a good model to work with in this type of problem. <br>\n",
    "\n",
    "You have the data, lets start working on the model!\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "Details of the full list of features is given below:\n",
    "\n",
    "Notes\n",
    "------\n",
    "Data Set Characteristics:  \n",
    "\n",
    "    :Number of Instances: 506 \n",
    "\n",
    "    :Number of Attributes: 13 numeric/categorical predictive\n",
    "    \n",
    "    :Median Value (attribute 14) is usually the target\n",
    "\n",
    "    :Attribute Information (in order):\n",
    "        - CRIM     per capita crime rate by town\n",
    "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        - INDUS    proportion of non-retail business acres per town\n",
    "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "        - NOX      nitric oxides concentration (parts per 10 million)\n",
    "        - RM       average number of rooms per dwelling\n",
    "        - AGE      proportion of owner-occupied units built prior to 1940\n",
    "        - DIS      weighted distances to five Boston employment centres\n",
    "        - RAD      index of accessibility to radial highways\n",
    "        - TAX      full-value property-tax rate per $10,000 \n",
    "        - PTRATIO  pupil-teacher ratio by town\n",
    "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        - LSTAT    % lower status of the population\n",
    "        - price     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "\n",
    "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
    "\n",
    "This is a copy of UCI ML housing dataset.\n",
    "http://archive.ics.uci.edu/ml/datasets/Housing\n",
    "\n",
    "\n",
    "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
    "\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    "prices and the demand for clean air', J. Environ. Economics & Management,\n",
    "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
    "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
    "pages 244-261 of the latter.\n",
    "\n",
    "The Boston house-price data has been used in many machine learning papers that address regression\n",
    "problems.   \n",
    "     \n",
    "**References**\n",
    "\n",
    "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
    "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
    "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import the libraries we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are already familiar with these ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are already familiar with these ones!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create a pandas dataframe by loading the data from a CSV File :** Boston_housing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Data/Boston_housing.csv')\n",
    "# df.columns = ['CRIME', 'ZONE', 'RETAILPERC', 'RIVER', 'NO2', 'ROOMS', 'AGE', 'EMPCTRDIST', \n",
    "#               'HIGHWAYACCS', 'TAX', 'TEACHERRATIO', \n",
    "#              'BKPROP', 'POORPERC', 'PRICE']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some information on the data, using info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick view on some of the statistical information of our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some Exploratory Data Analysis (EDA)\n",
    "**It is very important to know the data, let's see how the data look like.**<br>\n",
    "Let's see how the price is distributed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "g=sns.distplot(df['price'], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in we look at the histogram, most of the prices are in 20's. We notice that, the average price is around ~ 22K. There are some houses in 50's as well. <br>If you want, you can drop these rows where the price is in 50's, at the moment, we are keeping all the data <br><br><br>\n",
    "**saborn's pairplot function is a good option to explore little more.** <br>\n",
    "&#9758; Although, we can plot all the feature but the pairplot would be very crowded. Let's plot some important features to see how they are related to each other!<br>\n",
    "\n",
    "We can check how the `Crime rate (CRIM)`, `No. of rooms (RM)`,`Age of the house (AGE)` , `% lower status of the population (LSTAT)`, ` weighted distances to five Boston employment centres (DIS)` and `price` are related to each other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime, No of rooms, Age of the house, Price\n",
    "sns.pairplot(df[['CRIM','RM','AGE','LSTAT','DIS','ZN','price']]) \n",
    "#sns.pairplot(df) # in case you want to plot whole dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relationship of Crime vs Price\n",
    "\n",
    "sns.jointplot(x=\"LSTAT\", y=\"price\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we give a quick look on the last row, price vs selected features, we see some trends. Let's see how the heatmap looks like for the selected features!<br><br><br>\n",
    "### Let's see how the correlation between selected features looks like using heatmap!<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "sns.heatmap(df[['ZN','CRIM','RM','AGE','LSTAT','DIS','price']].corr()\n",
    "            , annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, every column is perfectly correlated to itself along the diagonal!\n",
    "we can see +ve and -ve correlation between price and other features. No of rooms have the highest value for price, that make sense, more the rooms are, higher the price is where as older the house is, lower the price you can expect!<br><br><br>\n",
    "**You can spend more time on EDA, get more plots and see how much information you can get from your dataset. Our focus in this lecture is machine learning model and we will not spend more time on EDA. <br>**\n",
    "\n",
    "Let's create a model to suggest the house price based on the selected features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Rooms vs Price\n",
    "sns.set_style('dark')\n",
    "data = df.copy()\n",
    "data['RM']= np.round(data['RM'])\n",
    "g=sns.catplot(y='price', x='RM', data =data, kind='swarm', aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's move on the Machine Learning now using scikit-learn!**<br>\n",
    "The first thing is to separate the data into:<br>\n",
    "* X that will contain the selected features\n",
    "* y will be the target values, in this case price of the house.\n",
    "\n",
    "### X and y arrays\n",
    "We are only using the selected feature, so need to pass the column names.<br>\n",
    "&#9758; *I suggest, repeat the same model using all features once we are done with this lecture, that would be a good practice and you can also compare the results!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT']\n",
    "features = ['RM','LSTAT','PTRATIO']\n",
    "X = df[features]\n",
    "#X= df[]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model Training\n",
    "Excited!<br>\n",
    "Time to create/train our model!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train Test Split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the head of X_train, just for a quick look\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating and Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearRegression model is a part of linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9758; We got the output above, our `LinearRegression` model has been trained on the provided data to the model!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Evaluation\n",
    "Our model is trained, we need to evaluate our model. Let's evaluate the model by checking it's coefficients and how we can interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the intercept\n",
    "# The intercept (often labeled the constant) is the expected mean value of Y when all X=0.\n",
    "print(lm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Coefficients\n",
    "Coeffecients relates each feature in the dataset, each feature will have a separate coefficient!<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the coefficients from our model \n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**<br>\n",
    "We can create a dataframe using columns from `X` as `index` and values of the coefficients in a new column `'Coefficient'`. Its organized and look better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's discuss coefficients briefly.**<br>\n",
    "Let's take RM = 3.839684:<br>\n",
    "* This suggest, if we keep all other coefficients constant, a one unit increase in the RM is associated with an increase of 3.839684 in the price. <br>\n",
    "* The same is for other related coefficients. e.g. the Crime rate, age etc decreases the price according to their coefficients, keeping all other constants.  <br>\n",
    "\n",
    "If you want further detail and mathematics behind this, please read the suggested reading assignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predictions from our Model\n",
    "\n",
    "\n",
    "We have trained our model, discussed the coefficients which make some sense, now, its important to know how well the model is doing!<br>\n",
    "\n",
    "Our model have never seen `X_test`, let's provide test data \"`X_test`\" to our created model and see what the predictions are. Once we get the predictions from the model, we can compare them with what we have in our `y_test`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions from the model \n",
    "y_test_hat = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({'Actual': y_test,'Predictions': y_test_hat})\n",
    "scores.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the price of all homes with features in `X_test`, which is in `y_test`, let's plot `y_test` and predictions, scatter plot is a good option!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test_hat, y=y_test)\n",
    "sns.scatterplot(x=y_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Histogram can tell us how much the predicted value differ from the actual value in `y_test`. We can simple do the subtraction `y_test - predictions` for this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Evaluation Metrics\n",
    "\n",
    "\n",
    "Here are three common evaluation metrics for regression problems. All of these are **loss functions**, because we want to minimize them.<br>\n",
    "\n",
    "*Consider, ${y}$ is a vector of `n` predictions generated from a sample of `n` data points on all variables, and \n",
    "$\\hat{y}$ is the vector of observed values (target values) of the variable being predicted.<br>*\n",
    "\n",
    "**[Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error)** (MAE) is the mean of the absolute value of the errors: <br>\n",
    "it's the average error!\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i  |$$\n",
    "\n",
    "**[Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error)** (MSE) is the mean of the squared errors:<br>\n",
    "**MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**[Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)** (RMSE) is the square root of the mean of the squared errors:<br>\n",
    "**RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units. <br><br>The root-mean-squared error (**RMSE**) **or** root-mean-square deviation (**RMSD**), is a frequently used measure of the differences between values predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values.\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "<br><br><br>\n",
    "Let's calculate MAE, MSE and RMSE for our model. <br>We need to pass the y_test and predictions to the respective method!<br>\n",
    "\n",
    "We need to do another import here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Evaluation Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_test_hat))\n",
    "rSquared = r2_score(y_test, y_test_hat)\n",
    "print('RMSE:', rmse)\n",
    "print(\"Mean Price: \", df['price'].mean())\n",
    "print('R-squared:', rSquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 : Add all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT']\n",
    "#features = ['RM','LSTAT','PTRATIO']\n",
    "\n",
    "X = df[all_features]\n",
    "# Load the features to a variable X (note: we have 7 features now, so the first 7 columns need to be selected.)\n",
    "\n",
    "\n",
    "# Load y variable\n",
    "y = df['price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "#Instantiate the algorithm\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Training the model on our train dataset\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "# print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "# print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "print('R-squared' , metrics.r2_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING VARIOUS Regressors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3 : RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the features to a variable X (note: we have 7 features now, so the first 7 columns need to be selected.)\n",
    "X= df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT']]\n",
    "\n",
    "# Load y variable\n",
    "y = df['price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "#Instantiate the algorithm\n",
    "lr = RandomForestRegressor()\n",
    "\n",
    "# Training the model on our train dataset\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "# Get predictions\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "# print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "# print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "print('R-squared' , metrics.r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Version 4 : Trying many  algorithms together, with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Load the features to a variable X (note: we have 7 features now, so the first 7 columns need to be selected.)\n",
    "X= df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT']]\n",
    "\n",
    "# Load y variable\n",
    "y = df['price']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "resultdf = pd.DataFrame(columns=['Algo', 'MAE','RMSE', 'R2'])\n",
    "a = 0.3\n",
    "rowcnt=0\n",
    "\n",
    "for name,algo in [\n",
    "        ('Linear regression', LinearRegression()),\n",
    "        ('Lasso', Lasso(fit_intercept=True, alpha=a)),\n",
    "        ('Ridge', Ridge(fit_intercept=True, alpha=a)),\n",
    "        ('Elastic-net', ElasticNet(fit_intercept=True, alpha=a)),\n",
    "        ('SGD ',SGDRegressor(alpha=0.05,penalty='none')),\n",
    "        ('DecisionTreeRegressor',DecisionTreeRegressor(max_depth=4)),\n",
    "        ('Support Vector Linear',SVR(kernel='linear', C=1)),\n",
    "        ('Random Forest',RandomForestRegressor()),\n",
    "        ('Gradient Boosting',GradientBoostingRegressor())\n",
    "        ]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "    #Instantiate the algorithm\n",
    "    lr = algo\n",
    "\n",
    "    # Training the model on our train dataset\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = lr.predict(X_test)\n",
    "    mae=metrics.mean_absolute_error(y_test, predictions)\n",
    "    rmse=np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
    "    r2=metrics.r2_score(y_test, predictions)\n",
    "                 \n",
    "    resultdf.loc[rowcnt]=[name,mae,rmse,r2]\n",
    "    rowcnt=rowcnt+1\n",
    "    print('Model ',name)\n",
    "    #print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "    #print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "    print('R-squared' , metrics.r2_score(y_test, predictions))\n",
    "    print('-----------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf.sort_values(by='R2', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations !  End of Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Material - Using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "          loss='mse',\n",
    "          metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model.fit(X_train, y_train, epochs=80, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "print('R-squared' , metrics.r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video on Bias and Variance \n",
    "\n",
    "https://www.youtube.com/watch?v=EuBBz3bI-aA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
